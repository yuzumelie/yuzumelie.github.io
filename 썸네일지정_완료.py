# -*- coding: utf-8 -*-
"""ì¸ë„¤ì¼ì§€ì •_ì™„ë£Œ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-N2lFVDJ4CSMz1G_LfSh6cwSVfjw6Y0Y
"""

#@title 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì²« 1íšŒë§Œ ì‹¤í–‰)
# [ìˆ˜ì •] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒ í•´ê²° (protobuf ë²„ì „ 4.xë¡œ ê°•ì œ ë‹¤ìš´ê·¸ë ˆì´ë“œ)
!pip install --upgrade --force-reinstall google-cloud-vision opencv-python-headless "protobuf<5.0.0"

#@title 2. Google Cloud ì¸ì¦
# (Cloud Vision APIê°€ í™œì„±í™”ëœ Google ê³„ì •ìœ¼ë¡œ ì¸ì¦í•˜ì„¸ìš”)
from google.colab import auth
print("Google Cloud ê³„ì •ìœ¼ë¡œ ì¸ì¦ì„ ì§„í–‰í•©ë‹ˆë‹¤...")
auth.authenticate_user()
print("ì¸ì¦ ì™„ë£Œ!")

#@title 3. [ì‹ ê·œ] Google Drive ë§ˆìš´íŠ¸
# (í…ŒìŠ¤íŠ¸ìš© ì˜ìƒì„ Driveì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤)
from google.colab import drive
import os
drive.mount('/content/drive')
print("\nâœ… Google Driveê°€ '/content/drive'ì— ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("ë‹¤ìŒ ì…€ì—ì„œ í…ŒìŠ¤íŠ¸í•  ì˜ìƒ íŒŒì¼ì˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

#@title 4. ë°±ì—”ë“œ AI ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ìµœì  ì¸ë„¤ì¼ ìƒì„±)

from google.cloud import vision
import cv2
import os
import numpy as np # numpy ì„í¬íŠ¸
from google.colab.patches import cv2_imshow
import math

# Google Cloud í”„ë¡œì íŠ¸ ID ì„¤ì •
PROJECT_ID = "braided-liberty-476215-v4"

# --- 1. Vision AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
try:
    client_options = {"quota_project_id": PROJECT_ID}
    vision_client = vision.ImageAnnotatorClient(client_options=client_options)
    print(f"âœ… Vision AI í´ë¼ì´ì–¸íŠ¸ê°€ '{PROJECT_ID}' í”„ë¡œì íŠ¸ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ Vision AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

# --- 2. 1ì´ˆ ê°„ê²© í”„ë ˆì„ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_frames_by_interval(video_path, sec_per_frame=1.0):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ì„¤ì •ëœ ì‹œê°„(ì´ˆ) ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    print(f"\n'{video_path}'ì—ì„œ {sec_per_frame}ì´ˆ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ ì¤‘...")
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return []

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps * sec_per_frame)
    if frame_interval == 0: frame_interval = 1

    frames_data = []
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            ret_enc, buffer = cv2.imencode('.jpg', frame)
            if ret_enc:
                current_time_sec = frame_count / fps
                frames_data.append({
                    'time_sec': current_time_sec,
                    'image_bytes': buffer.tobytes(),
                    'image_cv2': frame # ì‹œê°ì  í™•ì¸ì„ ìœ„í•´ cv2 ì´ë¯¸ì§€ë„ ì €ì¥
                })

        frame_count += 1

    cap.release()
    print(f"âœ… ì´ {len(frames_data)}ê°œì˜ í”„ë ˆì„ì´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return frames_data

# --- 3. [ì‹ ê·œ] Vision AI ì–¼êµ´ ë¶„ì„ ë° ì ìˆ˜í™” í•¨ìˆ˜ ---
# Vision AIê°€ ë°˜í™˜í•˜ëŠ” Likelihood ë¬¸ìì—´ì„ ì ìˆ˜ë¡œ ë³€í™˜
LIKELIHOOD_SCORE = {
    'UNKNOWN': 0,
    'VERY_UNLIKELY': 0,
    'UNLIKELY': 1,
    'POSSIBLE': 2,
    'LIKELY': 4,
    'VERY_LIKELY': 5
}
# [ì‚­ì œ] LIKELIHOOD_STRING (ë””ë²„ê¹…ìš©) ì‚­ì œ


def analyze_frame_for_thumbnail(image_bytes):
    """
    ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ Vision AIë¡œ ì „ì†¡í•˜ì—¬ ì–¼êµ´ì„ ë¶„ì„í•˜ê³  ì¸ë„¤ì¼ ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
    """
    image = vision.Image(content=image_bytes)

    response = vision_client.face_detection(image=image)
    faces = response.face_annotations

    if not faces:
        return 0, "ì–¼êµ´ ì—†ìŒ", "N/A" # ì ìˆ˜, ìƒíƒœ, mouth_info

    best_face_score = -999
    best_mouth_info = "N/A"

    for face in faces:
        # [ìˆ˜ì •] ì ìˆ˜ ë¡œì§ ì „ë©´ ìˆ˜ì •: 0ì ì—ì„œ ì‹œì‘
        score = 0

        # 1. ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ (ì„ ëª…í•¨, ë°ìŒ, ì •ë©´, ë†’ì€ ì‹ ë¢°ë„)
        if LIKELIHOOD_SCORE.get(face.blurred_likelihood, 0) < 3: score += 50
        if LIKELIHOOD_SCORE.get(face.under_exposed_likelihood, 0) < 3: score += 20
        if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20: score += 30
        if face.detection_confidence > 0.7: score += 20

        # 2. ìš°ì„ ìˆœìœ„ ê°€ì‚°ì  (P1 > P2 > P3)
        # [ì‚­ì œ] joy_score_val (API ê¸°ì¨) ê´€ë ¨ ë¡œì§ ëª¨ë‘ ì‚­ì œ

        # [ìˆ˜ì •] ëœë“œë§ˆí¬ ê¸°ë°˜ P1(ì›ƒìŒ), P2(ì…ë‹¤ë­„) ì ìˆ˜ ê³„ì‚°
        landmarks = {lm.type_: lm.position for lm in face.landmarks}
        mouth_openness = 0.0
        smile_pull = 0.0

        if (vision.FaceAnnotation.Landmark.Type.UPPER_LIP in landmarks and
            vision.FaceAnnotation.Landmark.Type.LOWER_LIP in landmarks):
            mouth_openness = abs(landmarks[vision.FaceAnnotation.Landmark.Type.UPPER_LIP].y -
                                 landmarks[vision.FaceAnnotation.Landmark.Type.LOWER_LIP].y)

        if (vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER in landmarks and
            vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT in landmarks and
            vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT in landmarks):
            center_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER].y
            left_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT].y
            right_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT].y
            # ì…ê¼¬ë¦¬ê°€ ì¤‘ì•™ë³´ë‹¤ ìœ„ë¡œ ì˜¬ë¼ê°ˆìˆ˜ë¡ ì ìˆ˜ê°€ ë†’ìŒ (ì–‘ìˆ˜)
            smile_pull = (center_y - left_y) + (center_y - right_y)

        mouth_info_str = f"Open:{mouth_openness:.2f}, Pull:{smile_pull:.2f}"

        # [ìˆ˜ì •] ëœë“œë§ˆí¬ë§Œìœ¼ë¡œ P1/P2/P3 ë¡œì§ í†µí•©
        # P1: ëœë“œë§ˆí¬ê°€ 'ì›ƒìŒ'ì´ë¼ í•¨(pull>0.03)
        if smile_pull > 0.03:
            score += 150 # P1 (ë¯¸ì†Œ/ì›ƒìŒ)
            if smile_pull > 0.1: # í™œì§ ì›ƒìŒ
                score += 150 # ì¶”ê°€ ì ìˆ˜

        # P2: P1ì´ ì•„ë‹Œ ê²½ìš° ì¤‘, ì…ì„ ë‹¤ë¬¸ ê²½ìš°
        elif mouth_openness <= 0.1:
             score += 100 # P2 (ì… ë‹¤ë¬¸ ì–¼êµ´)

        # P3: P1, P2 ëª¨ë‘ ì•„ë‹Œ ê²½ìš° (ì… ë²Œë¦° ì–¼êµ´)
        # (ê°€ì‚°ì  ì—†ìŒ. ê¸°ë³¸ ì ìˆ˜ 120ì  ê·¼ì²˜)

        if score > best_face_score:
            best_face_score = score
            best_mouth_info = mouth_info_str

    return best_face_score, "ì–¼êµ´ ìˆìŒ", best_mouth_info


# --- 4. ë©”ì¸ ë¡œì§ ì‹¤í–‰ í•¨ìˆ˜ (ìµœì  ì¸ë„¤ì¼ ì„ ì •) ---
def find_best_thumbnail(video_path):
    """
    ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ì—¬ 'ë² ìŠ¤íŠ¸ ì¸ë„¤ì¼' 1ê°œë¥¼ ì„ ì •í•©ë‹ˆë‹¤.
    """
    if 'vision_client' not in globals():
         print("âŒ ì˜¤ë¥˜: Vision AI í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
         return

    # [ìˆ˜ì •] í”„ë ˆì„ ì¶”ì¶œ ê°„ê²©ì„ 0.25ì´ˆë¡œ ì„¸ë¶„í™” (1ì´ˆì— 4ì¥)
    frames = extract_frames_by_interval(video_path, sec_per_frame=0.25)
    if not frames:
        return

    print(f"\nGoogle Vision AIë¡œ ê° í”„ë ˆì„ ë¶„ì„ ì‹œì‘ (ì´ {len(frames)}ê°œ)...")

    scored_frames = []

    # 2. ê° í”„ë ˆì„ì„ Vision AIë¡œ ë¶„ì„ ë° ì ìˆ˜í™”
    for i, frame_data in enumerate(frames):
        # [ìˆ˜ì •] joy_str ì œê±°
        score, status, mouth_info_str = analyze_frame_for_thumbnail(frame_data['image_bytes'])
        frame_data['score'] = score
        frame_data['status'] = status
        frame_data['mouth'] = mouth_info_str
        scored_frames.append(frame_data)

        # [ìˆ˜ì •] ë¡œê·¸ê°€ ë„ˆë¬´ ë§ì•„ì§€ë¯€ë¡œ, 20í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ì¶œë ¥
        if i % 20 == 0: # ì•½ 5ì´ˆë§ˆë‹¤
            if status == "ì–¼êµ´ ìˆìŒ":
                # [ìˆ˜ì •] ë””ë²„ê¹… ë¡œê·¸ ìƒì„¸í™” (API ê¸°ì¨ ì œê±°)
                print(f"  [ì‹œê°„: {frame_data['time_sec']:.2f}s] âœ… ì–¼êµ´ ê°ì§€ë¨ (ì ìˆ˜: {int(score)}) ({mouth_info_str})")
            else:
                print(f"  [ì‹œê°„: {frame_data['time_sec']:.2f}s] ì–¼êµ´ ì—†ìŒ (ì ìˆ˜: {int(score)})")

    scored_frames.sort(key=lambda x: x['score'], reverse=True)

    print("\n" + "="*30)
    print("ğŸ‰ AI ë¶„ì„ ì™„ë£Œ! (ìµœì  ì¸ë„¤ì¼)")
    print("="*30)

    # [ë””ë²„ê¹…] ìƒìœ„ 5ê°œ í”„ë ˆì„ ì ìˆ˜ ì¶œë ¥
    print("--- ìƒìœ„ 5ê°œ í”„ë ˆì„ ì ìˆ˜ ---")
    for i in range(min(5, len(scored_frames))):
        frame = scored_frames[i]
        # [ìˆ˜ì •] API ê¸°ì¨ ì œê±°
        print(f"  Top {i+1}: [ì‹œê°„: {frame['time_sec']:.2f}s] (ì ìˆ˜: {int(frame['score'])}) ({frame['mouth']})")

    best_thumbnail = scored_frames[0]

    if best_thumbnail['score'] <= 0:
        print("\nê²°ê³¼: ğŸŒ„ ì´ ì˜ìƒì—ëŠ” ìœ ì˜ë¯¸í•œ ì–¼êµ´ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("      (ìš°ì„ ìˆœìœ„ 3) ì˜ìƒì˜ 50% ì§€ì  í”„ë ˆì„ì„ ëœë¤ ì¸ë„¤ì¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.")
        middle_frame = frames[len(frames) // 2]
        best_thumbnail = middle_frame
    else:
        print(f"\nê²°ê³¼: ğŸ˜ƒ AIê°€ 'ë² ìŠ¤íŠ¸ ì¸ë„¤ì¼'ì„ ì„ ì •í–ˆìŠµë‹ˆë‹¤! (ì ìˆ˜: {int(best_thumbnail['score'])})")

    print("\n--- [ìµœì¢… ê²°ê³¼] AI ìë™ ì§€ì • ì¸ë„¤ì¼ (Best Pick) ---")
    print(f"(ì„ ì • ì‹œê°„: {best_thumbnail['time_sec']:.2f}ì´ˆ)")
    cv2_imshow(best_thumbnail['image_cv2'])

#@title 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# (Vision APIê°€ í™œì„±í™”ëœ ìƒíƒœì—ì„œ ì´ ì…€ì„ ì‹¤í–‰í•©ë‹ˆë‹¤)
my_video_path = "/content/drive/MyDrive/video/thumbnail.MOV"

if 'PROJECT_ID' in globals() and PROJECT_ID:
    if os.path.exists(my_video_path):
        find_best_thumbnail(my_video_path)
    else:
        print(f"âŒ ì˜¤ë¥˜: í…ŒìŠ¤íŠ¸í•  ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"  -> ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {my_video_path}")
else:
    print("âŒ ì˜¤ë¥˜: '#@title 4' ì…€ì— ë³¸ì¸ì˜ Google Cloud í”„ë¡œì íŠ¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")